{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip install transformers==4.44.2"
      ],
      "metadata": {
        "id": "0SlLKhFYkeXE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initiate the Model and Tokenizer using Unsloth\n",
        "\n",
        "Here we use a custom model that was trained by me using unsloth on Salesforce/xlam-function-calling-60k dataset. The model is trained with a lora adapter. Which means that for function calling we can use the model with the adapter and for the thinking and reasoning we can use the model without the adapter. This saves memory. We can do a similar thing with bigger models like llama3.2-8b which already have function calling in-built."
      ],
      "metadata": {
        "id": "ef3zJGKntSeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"akshayballal/phi-3.5-mini-xlam-function-calling\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model);"
      ],
      "metadata": {
        "id": "7r8D5DNCvNRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb50e1b2-289a-4166-986f-a5cccb47a78c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopping Criteria\n",
        "\n",
        "Add a stop sequence so that the generation of the agent stops at the word \"PAUSE\" which is there in the system prompt that we provide. This allows the agent to get the observation from the tool use.  "
      ],
      "metadata": {
        "id": "035_KaL7s9yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "import torch\n",
        "\n",
        "class KeywordsStoppingCriteria(StoppingCriteria):\n",
        "    def __init__(self, keywords_ids:list):\n",
        "        self.keywords = keywords_ids\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, _: torch.FloatTensor, **kwargs) -> bool:\n",
        "        if input_ids[0][-1] in self.keywords:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "stop_ids = [17171]\n",
        "stop_criteria = KeywordsStoppingCriteria(stop_ids)"
      ],
      "metadata": {
        "id": "WjwPhGI9NxUh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the Tools\n",
        "\n",
        "Here we write the functions that we want the agent to use. These functions are written in a way that they can be used for function calling. Essentially the function should have parameters that are the input to the function and the return type should be the output of the function. There needs to be a docstring that describes the function."
      ],
      "metadata": {
        "id": "uwxLo2vi_NAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install embed_anything\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guHMDvFhI2SO",
        "outputId": "37338d7f-7b37-4d21-a8fb-2ec06cd30d2a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting embed_anything\n",
            "  Downloading embed_anything-0.4.10-cp310-cp310-manylinux_2_34_x86_64.whl.metadata (12 kB)\n",
            "Downloading embed_anything-0.4.10-cp310-cp310-manylinux_2_34_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: embed_anything\n",
            "Successfully installed embed_anything-0.4.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from embed_anything.vectordb import Adapter\n",
        "\n",
        "from embed_anything import EmbedData, EmbeddingModel, WhichModel, TextEmbedConfig\n"
      ],
      "metadata": {
        "id": "IVG6Vk2gHdsz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_numbers(a: int, b: int) -> int:\n",
        "    \"\"\"\n",
        "    This function takes two integers and returns their sum.\n",
        "\n",
        "    Parameters:\n",
        "    a (int): The first integer to add.\n",
        "    b (int): The second integer to add.\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "def square_number(a: int) -> int:\n",
        "    \"\"\"\n",
        "    This function takes an integer and returns its square.\n",
        "\n",
        "    Parameters:\n",
        "    a (int): The integer to be squared.\n",
        "    \"\"\"\n",
        "    return a * a\n",
        "\n",
        "def square_root_number(a: int) -> int:\n",
        "    \"\"\"\n",
        "    This function takes an integer and returns its square root.\n",
        "\n",
        "    Parameters:\n",
        "    a (int): The integer to calculate the square root of.\n",
        "    \"\"\"\n",
        "    return a ** 0.5\n",
        "\n",
        "def RagAnything(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    This function takes a query and search for relevant information in the vector database and returns the answer.\n",
        "\n",
        "    Parameters:\n",
        "    a (str): The query to be answered.\n",
        "    \"\"\"\n",
        "    model = EmbeddingModel.from_pretrained_hf(\n",
        "    WhichModel.Bert, model_id=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "    config = TextEmbedConfig(chunk_size=256, batch_size=32, buffer_size  = 64,splitting_strategy = \"sentence\")\n",
        "\n",
        "    embed_query = embed_anything.embed_query([prompt], embeder=model)\n",
        "    result = index.query(\n",
        "        vector=embed_query[0].embedding,\n",
        "        top_k=2,\n",
        "        include_metadata=True,\n",
        "    )\n",
        "    print(result)\n",
        "    metadata_texts = [item['metadata']['text'] for item in result['matches']]\n",
        "    return metadata_texts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lw13LB84_Od6"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [add_numbers, square_number, square_root_number, RagAnything] # add the tools to a list"
      ],
      "metadata": {
        "id": "-C2hBhjZ_QpK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Tool Descriptions\n",
        "\n",
        "We generate the tool descriptions in a way that is easy for the agent to understand and use. This is in the form of a list of dictionaries.\n"
      ],
      "metadata": {
        "id": "unHlN9-5_Szk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool_descriptions = []\n",
        "for tool in tools:\n",
        "    spec = {\n",
        "        \"name\": tool.__name__,\n",
        "        \"description\": tool.__doc__.strip(),\n",
        "        \"parameters\": [\n",
        "            {\n",
        "                \"name\": param,\n",
        "                \"type\": arg.__name__ if hasattr(arg, '__name__') else str(arg),\n",
        "            } for param, arg in tool.__annotations__.items() if param != 'return'\n",
        "        ]\n",
        "    }\n",
        "    tool_descriptions.append(spec)\n",
        "tool_descriptions\n"
      ],
      "metadata": {
        "id": "EijXwN20_UOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b05dbf-3c3a-4016-930c-ca515ed0d596"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'add_numbers',\n",
              "  'description': 'This function takes two integers and returns their sum.\\n\\n    Parameters:\\n    a (int): The first integer to add.\\n    b (int): The second integer to add.',\n",
              "  'parameters': [{'name': 'a', 'type': 'int'}, {'name': 'b', 'type': 'int'}]},\n",
              " {'name': 'square_number',\n",
              "  'description': 'This function takes an integer and returns its square.\\n\\n    Parameters:\\n    a (int): The integer to be squared.',\n",
              "  'parameters': [{'name': 'a', 'type': 'int'}]},\n",
              " {'name': 'square_root_number',\n",
              "  'description': 'This function takes an integer and returns its square root.\\n\\n    Parameters:\\n    a (int): The integer to calculate the square root of.',\n",
              "  'parameters': [{'name': 'a', 'type': 'int'}]},\n",
              " {'name': 'RagAnything',\n",
              "  'description': 'This function takes a query and search for relevant information in the vector database and returns the answer.\\n\\n    Parameters:\\n    a (str): The query to be answered.',\n",
              "  'parameters': [{'name': 'prompt', 'type': 'str'}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the Agent Class\n",
        "\n",
        "We then create the agent class that takes the system prompt, the function calling prompt, the tools and the messages as input and returns the response from the agent.\n",
        "\n",
        "- `__call__` is the function that is called when the agent is called with a message. It adds the message to the messages list and returns the response from the agent.\n",
        "- `execute` is the function that is called to generate the response from the agent. It uses the model to generate the response.\n",
        "- `function_call` is the function that is called to generate the response from the agent. It uses the function calling model to generate the response.\n",
        "\n"
      ],
      "metadata": {
        "id": "7R4i57q016Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self, system: str = \"\", function_calling_prompt: str = \"\", tools=[]\n",
        "    ) -> None:\n",
        "        self.system = system\n",
        "        self.tools = tools\n",
        "        self.function_calling_prompt = function_calling_prompt\n",
        "        self.messages: list = []\n",
        "        if self.system:\n",
        "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
        "\n",
        "    def __call__(self, message=\"\"):\n",
        "        if message:\n",
        "            self.messages.append({\"role\": \"user\", \"content\": message})\n",
        "        result = self.execute()\n",
        "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
        "        return result\n",
        "\n",
        "    def execute(self):\n",
        "        with model.disable_adapter():  # disable the adapter for thinking and reasoning\n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                self.messages,\n",
        "                tokenize=True,\n",
        "                add_generation_prompt=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            output = model.generate(\n",
        "                input_ids=inputs,\n",
        "                max_new_tokens=128,\n",
        "                stopping_criteria=StoppingCriteriaList([stop_criteria]),\n",
        "            )\n",
        "            return tokenizer.decode(\n",
        "                output[0][inputs.shape[-1] :], skip_special_tokens=True\n",
        "            )\n",
        "\n",
        "    def function_call(self, message):\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": self.function_calling_prompt.format(\n",
        "                        tool_descriptions=tool_descriptions, query=message\n",
        "                    ),\n",
        "                }\n",
        "            ],\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        output = model.generate(input_ids=inputs, max_new_tokens=128, temperature=0.0)\n",
        "        prompt_length = inputs.shape[-1]\n",
        "\n",
        "        answer = ast.literal_eval(\n",
        "            tokenizer.decode(output[0][prompt_length:], skip_special_tokens=True)\n",
        "        )[\n",
        "            0\n",
        "        ]  # get the output of the function call model as a dictionary\n",
        "        print(answer)\n",
        "        tool_output = self.run_tool(answer[\"name\"], **answer[\"arguments\"])\n",
        "        return tool_output\n",
        "\n",
        "    def run_tool(self, name, *args, **kwargs):\n",
        "        for tool in self.tools:\n",
        "            if tool.__name__ == name:\n",
        "                return tool(*args, **kwargs)"
      ],
      "metadata": {
        "id": "sIm10tkO2vZi"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the System Prompt and Function Calling Prompt\n",
        "\n",
        "Here the system prompt is based on the ReAct pattern. The agent is asked to think about the question, reason about the question, determine the actions to be taken, pause to get the observation and finally give the answer. Also we define the function calling prompt which is used to call the functions."
      ],
      "metadata": {
        "id": "-yhwAzBr_pFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"\n",
        "You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "At the end of the loop you output an Answer\n",
        "Use Thought to describe your thoughts about the question you have been asked.\n",
        "Use Action to run one of the actions available to you - then return PAUSE.\n",
        "Observation will be the result of running those actions. Stop when you have the Answer.\n",
        "Your available actions are:\n",
        "\n",
        "{tools}\n",
        "\n",
        "\n",
        "\"\"\".strip()\n",
        "\n",
        "function_calling_prompt = \"\"\"\n",
        "You are a helpful assistant. Below are the tools that you have access to.  \\n\\n### Tools: \\n{tool_descriptions} \\n\\n### Query: \\n{query} \\n\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cRVeaR5M_l9e"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEZgsBIeU6I1",
        "outputId": "e33ed1c2-49fb-49a6-bdc7-e0493c36f745"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
            "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client\n",
            "Successfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "pc =  Pinecone(\"1f880c47-f636-4200-a76a-431ee51e2d48\")\n",
        "\n",
        "# Initialize the PineconeEmbedder class\n",
        "index = pc.Index(\"anything\")\n"
      ],
      "metadata": {
        "id": "hALp3nRRTXY7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def loop_agent(agent: Agent, question, max_iterations=5):\n",
        "\n",
        "    next_prompt = question\n",
        "    i = 0\n",
        "    while i < max_iterations:\n",
        "        result = agent(next_prompt)\n",
        "        print(result)\n",
        "        if \"Answer:\" in result:\n",
        "            return result\n",
        "\n",
        "        action = re.findall(r\"Action: (.*)\", result)\n",
        "        if action:\n",
        "            tool_output= agent.function_call(action)\n",
        "            next_prompt = f\"Observation: {tool_output}\"\n",
        "            print(next_prompt)\n",
        "        else:\n",
        "            next_prompt = \"Observation: tool not found\"\n",
        "        i += 1\n",
        "    return result\n",
        "\n",
        "\n",
        "agent = Agent( system=system_prompt, function_calling_prompt=function_calling_prompt, tools=tools)\n",
        "\n",
        "loop_agent(agent, \"what is attention?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4JVKFadt233S",
        "outputId": "b0409a6a-1682-46db-acfb-b5f164454855"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: To answer this question, I need to understand the concept of attention in a cognitive or psychological context.\n",
            "\n",
            "Action: Use the function RagAnything to search for a general definition of attention.\n",
            "\n",
            "PAUSE\n",
            "{'name': 'RagAnything', 'arguments': {'prompt': 'What is attention?'}}\n",
            "{'matches': [{'id': '2596216e-f4a5-471b-9487-d0378655de6b',\n",
            "              'metadata': {'file': 'attention.pdf',\n",
            "                           'text': 'Where the projections are parameter '\n",
            "                                   'matrices W Q\\n'\n",
            "                                   'i ∈ Rdmodel×dk , W K\\n'\n",
            "                                   'i ∈ Rdmodel×dk , W V\\n'\n",
            "                                   'i ∈ Rdmodel×dv\\n'\n",
            "                                   'and W O ∈ Rhdv×dmodel.\\n'\n",
            "                                   '\\n'\n",
            "                                   'In this work we employ h = 8 parallel '\n",
            "                                   'attention layers, or heads. For each of '\n",
            "                                   'these we use\\n'\n",
            "                                   'dk = dv = dmodel/h = 64. Due to the '\n",
            "                                   'reduced dimension of each head, the total '\n",
            "                                   'computational cost\\n'\n",
            "                                   'is similar to that of single-head '\n",
            "                                   'attention with full dimensionality.\\n'\n",
            "                                   '\\n'\n",
            "                                   '3.2.3 Applications of Attention in our '\n",
            "                                   'Model\\n'\n",
            "                                   '\\n'\n",
            "                                   'The Transformer uses multi-head attention '\n",
            "                                   'in three different ways:\\n'\n",
            "                                   '\\n'\n",
            "                                   '• In \"encoder-decoder attention\" layers, '\n",
            "                                   'the queries come from the previous decoder '\n",
            "                                   'layer,\\n'\n",
            "                                   'and the memory keys and values come from '\n",
            "                                   'the output of the encoder. This allows '\n",
            "                                   'every\\n'\n",
            "                                   'position in the decoder to attend over all '\n",
            "                                   'positions in the input sequence. This '\n",
            "                                   'mimics the\\n'\n",
            "                                   'typical encoder-decoder attention '\n",
            "                                   'mechanisms in sequence-to-sequence models '\n",
            "                                   'such as\\n'\n",
            "                                   '[38, 2, 9].'},\n",
            "              'score': 0.248323068,\n",
            "              'values': []},\n",
            "             {'id': 'eb408830-c1b9-4577-9d25-b34e3c4417fd',\n",
            "              'metadata': {'file': 'attention.pdf',\n",
            "                           'text': 'End-to-end memory networks are based on a '\n",
            "                                   'recurrent attention mechanism instead of '\n",
            "                                   'sequence-\\n'\n",
            "                                   'aligned recurrence and have been shown to '\n",
            "                                   'perform well on simple-language question '\n",
            "                                   'answering and\\n'\n",
            "                                   'language modeling tasks [34].\\n'\n",
            "                                   '\\n'\n",
            "                                   'To the best of our knowledge, however, the '\n",
            "                                   'Transformer is the first transduction '\n",
            "                                   'model relying\\n'\n",
            "                                   'entirely on self-attention to compute '\n",
            "                                   'representations of its input and output '\n",
            "                                   'without using sequence-\\n'\n",
            "                                   'aligned RNNs or convolution. In the '\n",
            "                                   'following sections, we will describe the '\n",
            "                                   'Transformer, motivate\\n'\n",
            "                                   'self-attention and discuss its advantages '\n",
            "                                   'over models such as [17, 18] and [9].\\n'\n",
            "                                   '\\n'\n",
            "                                   '3 Model Architecture\\n'\n",
            "                                   '\\n'\n",
            "                                   'Most competitive neural sequence '\n",
            "                                   'transduction models have an '\n",
            "                                   'encoder-decoder structure [5, 2, 35].\\n'\n",
            "                                   'Here, the encoder maps an input sequence '\n",
            "                                   'of symbol representations (x1, ..., xn) to '\n",
            "                                   'a sequence\\n'\n",
            "                                   'of continuous representations z = (z1, '\n",
            "                                   '..., zn). Given z, the decoder then '\n",
            "                                   'generates an output\\n'\n",
            "                                   'sequence (y1, ..., ym) of symbols one '\n",
            "                                   'element at a time. At each step the model '\n",
            "                                   'is auto-regressive\\n'\n",
            "                                   '[10], consuming the previously generated '\n",
            "                                   'symbols as additional input when '\n",
            "                                   'generating the next.\\n'\n",
            "                                   '\\n'\n",
            "                                   '2'},\n",
            "              'score': 0.247559771,\n",
            "              'values': []}],\n",
            " 'namespace': '',\n",
            " 'usage': {'read_units': 6}}\n",
            "Observation: ['Where the projections are parameter matrices W Q\\ni ∈ Rdmodel×dk , W K\\ni ∈ Rdmodel×dk , W V\\ni ∈ Rdmodel×dv\\nand W O ∈ Rhdv×dmodel.\\n\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n\\n3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n\\n3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n\\n2']\n",
            "Thought: The provided text gives a detailed explanation of attention mechanisms, particularly in the context of the Transformer model. Attention, in this context, seems to be a method used in sequence-to-sequence models to allow each position in one sequence to attend to all positions in another sequence.\n",
            "\n",
            "Answer: Attention, in the context of sequence-to-sequence models like the Transformer, is a mechanism that allows every position in the decoder to attend over all positions in the input sequence. This is used in \"encoder-decoder attention\" layers, where the queries come from the previous decoder layer\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Thought: The provided text gives a detailed explanation of attention mechanisms, particularly in the context of the Transformer model. Attention, in this context, seems to be a method used in sequence-to-sequence models to allow each position in one sequence to attend to all positions in another sequence.\\n\\nAnswer: Attention, in the context of sequence-to-sequence models like the Transformer, is a mechanism that allows every position in the decoder to attend over all positions in the input sequence. This is used in \"encoder-decoder attention\" layers, where the queries come from the previous decoder layer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nbconvert"
      ],
      "metadata": {
        "id": "FIM5nNYk3dSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03340cb9-8693-4d04-bc8d-ca0305323771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nbconvert: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WExJbkWyBszn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}